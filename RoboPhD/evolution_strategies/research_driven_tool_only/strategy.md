# Research-Driven Strategy (Tool-Only Focus)

Having reviewed the files provided to you above, you are refining a Claude Code agent that uses a three-artifact architecture with the goal of achieving higher accuracy than any previous agent.

**IMPORTANT: This strategy REQUIRES implementing research insights through tool-only execution** where deterministic Python/shell scripts generate complete database analysis files. This combines academic research with the benefits of:
- **Consistency**: Same database always produces same analysis
- **Speed**: No LLM invocation costs or latency for Phase 1
- **Debuggability**: Clear, inspectable code implementing research techniques
- **Reliability**: Deterministic behavior without prompt sensitivity

## Context
You're evolving a database analysis system that consists of three distinct artifacts:
1. **eval_instructions.md** - Static SQL generation instructions passed directly to the eval model
2. **agent.md** - Database analysis agent that examines specific databases and runs tools
3. **tools/** - Python/shell scripts for database analysis

## Required Research Reading

You MUST read exactly ONE paper from the available research in papers/:

### Paper Selection Strategy:
1. First, run the paper selection tool to choose your paper:
   ```bash
   python strategy_tools/select_research_paper.py
   ```
   This tool will:
   - Select the next unused paper from the strategy's pool
   - Track used papers in strategy_tools/papers_pool.json
   - Write the selected paper path to `selected_paper.txt` in the current directory

2. Read the selected paper from the path provided by the tool

3. The tool tracks which papers have been used (when --agents-directory is specified) to ensure comprehensive exploration of all available methods

The papers are from top BIRD methods achieving 71-77% accuracy.

## Your Mission

Create an evolved three-artifact agent package that improves SQL generation accuracy by incorporating proven techniques from academic research **implemented through tool-only execution**.

### Tool-Only Implementation of Research Insights:

When reading the paper, think about:
- **Which techniques can be encoded as deterministic algorithms?**

- **How can paper insights inform tool design?**
  - Research may describe a multi-step reasoning process
  - You can implement each step as a Python function
  - Generate comprehensive output that captures all insights

- **What belongs in the tool vs eval_instructions?**
  - Tool: Database-specific analysis
  - eval_instructions: General SQL generation principles from research

### Creative Adaptation:
You're not limited to direct implementation. Consider:
- Combining techniques from multiple sections of the paper
- Simplifying complex research methods into practical Python algorithms
- Creating hybrid approaches that blend paper techniques with existing success patterns
- Inventing new tools inspired by but not directly copying research methods

### How to think about the research:
- The key focus is on novel techniques that achieve high BIRD scores
- **Think about which techniques can be implemented as deterministic code**
- Some standard techniques in papers might be overlooked basics worth adding
- Not everything is practical (e.g., fine-tuning LLMs) - use your judgment
- **Implement key insights as tool-only solutions**

## Tool-Only Execution Mode

The system supports a **tool-only execution mode** where your Python/shell tool generates a complete analysis file that is directly copied to the agent output, bypassing the AI agent entirely. This is the REQUIRED approach for implementing research insights.

### How Tool-Only Works

**YAML Frontmatter** in agent.md:
```yaml
---
name: your-agent-name
description: Brief description (mention research paper influence)
execution_mode: tool_only
tool_command: python tools/research_informed_analyzer.py
tool_output_file: tool_output/schema_analysis.txt
---
```

**Execution Flow**:
1. System runs `tool_command` with 600-second timeout
2. System checks output file exists and >= 200 bytes
3. If successful: copies file to `output/agent_output.txt` (agent never called)
4. If failed: clears `tool_output/` and calls agent normally for error recovery

## Required Output Structure

You must create the following files:

### 1. reasoning.md
Your analysis of what to improve and why, based on:

#### Performance Analysis
- Review of system prompts from best performers
- Analysis of agent performance across databases
- Identification of strengths and weaknesses

#### Error Analysis
{if error_analyzer}- Summary of key findings from error_analysis_report.md (generated by {error_analyzer_agent})
- How analyzer findings influenced your design decisions
{else}- Analysis of error patterns from available error analysis artifacts (evaluation.json, error_analysis_report.md, etc.)
- How your analysis of error patterns influenced your design decisions
{endif}

#### Academic Paper Analysis
- Which paper you selected and its key contributions
- What you learned from the paper
- Ideas most applicable to the three-artifact architecture
- **How you will implement these insights as a tool-only solution**
- Adaptations or modifications needed to fit deterministic execution

### 2. eval_instructions.md
Complete SQL generation instructions for the eval model incorporating research insights. For example:
- SQL writing principles and patterns from the paper
- Column selection rules informed by research
- Error patterns to avoid (based on paper and your error analysis)
- Output format requirements (clean SQL, no markdown)
- Common SQL patterns discovered in research
- These instructions go DIRECTLY to the eval model

### 3. tools/
Analysis tools as Python (.py) or shell (.sh) scripts **implementing research techniques**:
- **PRIMARY GOAL**: Create a tool that implements paper insights and generates COMPLETE analysis
- The tool should encode research algorithms as Python functions
- Output to `tool_output/schema_analysis.txt` (or similar)
- When generating these tools, think about what information the eval model will need about this specific database
- Note that this database-specific information is distinct from the general instructions provided in eval_instructions.md
- Remember that the eval model will not have any information about the database beyond the information that your tool provides
- Add error handling with meaningful exit codes
- You can use standard Python libraries + sqlite3
- You should analyze the specific database found at ./database.sqlite

**Tool Architecture**: You can implement your design as either a single comprehensive script or multiple focused scripts (one per research technique). Both approaches are valid. Making your tools easily modifiable in future evolution rounds is an important secondary goal.

Example tool structure (research-informed, tool-only):
```python
#!/usr/bin/env python3
"""Database analyzer implementing insights from research paper."""

import sqlite3
import sys

def analyze_database(db_path: str, output_file: str):
    """Generate analysis using research-informed techniques."""

    try:
        conn = sqlite3.connect(db_path)
        output = []

        # Implement research techniques here
        # Based on insights from the paper you selected
        # Design algorithms that capture key ideas

        output.append("# DATABASE ANALYSIS\n")
        # ... add your research-informed analysis ...

        # Write output
        with open(output_file, 'w') as f:
            f.write('\n'.join(output))

        print(f"Research-informed analysis complete")
        conn.close()
        return 0

    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        return 1

if __name__ == "__main__":
    exit(analyze_database("database.sqlite", "tool_output/analysis.txt"))
```

### 4. agent.md
Database analysis agent with YAML frontmatter enabling tool-only execution:

Create an agent.md file with this structure:

```
---
name: your-agent-name
description: Research-driven approach implementing insights from [Paper Name]
execution_mode: tool_only
tool_command: python tools/your_analyzer.py
tool_output_file: tool_output/analysis.txt
---

# Your Agent Name (Tool-Only, Research-Informed)

This agent uses deterministic tool-only execution implementing insights from [Paper Name].

## Process

1. **Run Research-Informed Analysis Tool**
   - Execute: python tools/your_analyzer.py

2. **Read and Output Results**
   - Read the generated analysis from tool_output/analysis.txt
   - Write the complete output to ./output/agent_output.txt

## Error Recovery

If the tool fails:

1. Check database.sqlite exists
2. Verify Python environment has required libraries
3. Examine any error messages in tool_output/
4. Attempt to run the tool manually to see errors
5. Fall back to manual analysis if needed
```

**Notes for evolution AI:**
- Implement research insights as deterministic algorithms
- Let the paper guide what analysis to include
- Tool output should support the SQL generation strategies from research

## Success Metrics

Your evolved package should:
- Implement research insights as deterministic Python algorithms
- Use tool-only execution mode for fast, consistent Phase 1
- Generate comprehensive database analysis incorporating paper techniques
- Preserve good general-purpose SQL generation instructions (in eval_instructions.md)
- Address specific failure patterns from your analysis
- Be maintainable and debuggable
- Achieve higher accuracy through research-informed design

## Your overall goal: Push accuracy higher with research-informed tool-only execution

You are an expert in the field of Text2SQL. Use your knowledge of the field, your analysis of what is bringing accuracy down with current agents, and insights from academic research to build an agent package that will achieve higher accuracy than previous agents on a set of databases that you haven't seen before.

## Important Notes

- The final system prompt will be: [tool output] + [eval_instructions]
- Tool implements research techniques as Python code
- Tool has access to: database.sqlite
- Tool should write to: tool_output/schema_analysis.txt (or similar)
- If tool succeeds: output used directly (fast, $0.00 cost)
- If tool fails: agent called for error recovery
- Agent has access to: database.sqlite, tools/, tool_output/

Remember: **Think harder** than you normally would about this. Read the research paper carefully, understand the key insights, and **implement them as deterministic algorithms in Python**. This combines the best of academic research with the reliability and efficiency of tool-only execution.
