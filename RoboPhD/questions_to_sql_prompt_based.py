#!/usr/bin/env python3
"""
Simplified SQL generator that uses pre-generated system prompts.
This replaces the complex artifact-based system with direct prompt usage.
"""

import json
import argparse
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path
import os
import re
from datetime import datetime
import sys
import time
import random

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))
from utilities.cached_sql_executor import CachedSQLExecutor, SQLTimeoutError

# Import Phase 2 cache manager
try:
    from .phase2_cache_manager import Phase2CacheManager
except ImportError:
    from RoboPhD.phase2_cache_manager import Phase2CacheManager

# Import LLM provider abstraction
try:
    from .llm_providers import get_provider, parse_model_spec, get_model_pricing, LLMResponse
except ImportError:
    from RoboPhD.llm_providers import get_provider, parse_model_spec, get_model_pricing, LLMResponse

# Rate limit retry configuration
RATE_LIMIT_MAX_RETRIES = 5
RATE_LIMIT_BASE_DELAY = 60  # seconds

# Handle both module and script execution
try:
    from .config import (
        MAX_TOKENS, 
        API_KEY_ENV_VAR,
        SUPPORTED_MODELS,
        DEFAULT_MODEL
    )
except ImportError:
    # When run as a script, import from RoboPhD
    from RoboPhD.config import (
        MAX_TOKENS, 
        API_KEY_ENV_VAR,
        SUPPORTED_MODELS,
        DEFAULT_MODEL
    )

class PromptBasedSQLGenerator:
    def __init__(self, prompt_path: str, model: str = DEFAULT_MODEL, api_key: Optional[str] = None, use_evidence: bool = True, db_path: Optional[str] = None, sql_validation_timeout: int = 30, verification_retries: int = 2, temperature_strategy: str = "progressive", debug_log_probability: float = 0.0, debug_log_dir: Optional[Path] = None, cache_manager: Optional[Phase2CacheManager] = None, llm_call_timeout: int = 120):
        """
        Initialize SQL generator with a pre-generated system prompt.

        Args:
            prompt_path: Path to the system prompt file generated by subagent
            model: Model to use for SQL generation
            api_key: Optional API key
            use_evidence: Whether to include evidence in prompts (default True)
            db_path: Path to database file for SQL validation (optional)
            sql_validation_timeout: Timeout in seconds for SQL validation (default 30)
            verification_retries: Number of verification attempts (default 2, 0 = current behavior)
            temperature_strategy: Temperature strategy for retries ("progressive", "fixed", "adaptive")
            debug_log_probability: Probability (0.0-1.0) of logging API calls for debugging (default 0.0)
            debug_log_dir: Directory to write debug logs (optional)
            cache_manager: Optional Phase2CacheManager for caching API responses (default None)
            llm_call_timeout: Per-call LLM timeout in seconds (default 120, for local models)
        """
        # Load pre-generated system prompt
        with open(prompt_path, 'r') as f:
            self.system_prompt = f.read()

        # Store cache manager
        self.cache_manager = cache_manager

        # Parse model specification (handles provider/model format)
        self.provider_name, self.model_id = parse_model_spec(model)
        self.model_key = model  # Store original for logging
        self.is_local_model = self.provider_name != 'anthropic'

        # Initialize provider (handles API key internally for Anthropic)
        api_key = api_key or os.getenv(API_KEY_ENV_VAR)
        if self.provider_name == 'anthropic' and not api_key:
            raise ValueError(f"API key required for Anthropic. Set {API_KEY_ENV_VAR} or pass --api_key")

        self.llm_call_timeout = llm_call_timeout
        self.provider = get_provider(model, api_key, default_timeout=llm_call_timeout)
        self.model_name = self.provider.model_name

        # Get pricing (local models have $0 pricing)
        self.pricing = get_model_pricing(model)
        
        # Track token usage
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.cache_creation_tokens = 0
        self.cache_read_tokens = 0
        self.total_cost = 0.0
        self.use_evidence = use_evidence
        
        # SQL validation setup
        self.db_path = db_path
        self.sql_validation_timeout = sql_validation_timeout
        self.sql_executor = CachedSQLExecutor() if db_path else None

        # Verification settings
        self.verification_retries = verification_retries
        self.temperature_strategy = temperature_strategy

        # Debug logging settings
        self.debug_log_probability = debug_log_probability
        self.debug_log_dir = debug_log_dir

        # Track validation statistics
        self.validation_stats = {
            'total_generated': 0,
            'validation_attempted': 0,
            'validation_passed': 0,
            'validation_failed': 0,
            'validation_failed_empty': 0,
            'retry_attempted': 0,
            'retry_succeeded': 0,
            'timeout_errors': 0,
            # New verification stats
            'verification_attempted': 0,
            'verification_succeeded': 0,
            'verification_failed': 0,
            'verification_attempts_total': 0,
            # Enhanced tracking
            'verification_passed_immediately': 0,
            'verification_improved': 0,
            'total_api_calls': 0,
            'generation_api_calls': 0,
            'verification_api_calls': 0
        }

        # Question tracking for progress reporting
        self.current_question_num = None
        self.total_questions = None

    def _maybe_debug_log_api_call(
        self,
        api_call_type: str,  # "generation" or "verification"
        question: Dict,
        prompt: str,
        api_params: Dict,
        response: Optional[str] = None
    ):
        """
        Log API call with configured probability for debugging.

        Args:
            api_call_type: Type of API call ("generation" or "verification")
            question: Question dictionary
            prompt: Prompt being sent to API
            api_params: API parameters (model, temperature, etc.)
            response: Optional API response
        """
        if self.debug_log_probability == 0 or not self.debug_log_dir:
            return

        if random.random() >= self.debug_log_probability:
            return

        # Create debug directory
        debug_dir = Path(self.debug_log_dir)
        debug_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        qid = question.get('question_id', question.get('_original_index', 'unknown'))
        filename = f"api_call_{timestamp}_{qid}_{api_call_type}.json"

        # Truncate system_prompt if present in api_params
        api_params_copy = api_params.copy()
        if 'system_prompt' in api_params_copy:
            system_prompt = api_params_copy['system_prompt']
            total_chars = len(system_prompt)

            if total_chars > 400:
                # Truncate: first 200 + ... + last 200
                truncated = system_prompt[:200] + "\n\n... [TRUNCATED] ...\n\n" + system_prompt[-200:]
                api_params_copy['system_prompt'] = truncated
                api_params_copy['system_prompt_total_chars'] = total_chars
            else:
                # Short enough, keep as-is but add char count
                api_params_copy['system_prompt_total_chars'] = total_chars

        # Build debug data
        debug_data = {
            'timestamp': datetime.now().isoformat(),
            'api_call_type': api_call_type,
            'question_metadata': {
                'question_id': qid,
                'question': question.get('question'),
                'evidence': question.get('evidence'),
                'db_id': question.get('db_id')
            },
            'prompt': prompt,
            'api_parameters': api_params_copy,
            'response': response
        }

        # Write to file
        try:
            with open(debug_dir / filename, 'w') as f:
                json.dump(debug_data, f, indent=2)
        except Exception as e:
            # Silently ignore debug logging errors - don't break SQL generation
            pass

    def _get_db_name(self) -> str:
        """Extract database name from db_path for logging"""
        if self.db_path:
            return Path(self.db_path).stem
        return "unknown"

    def generate_sql(self, questions: List[Dict], db_name: str) -> Dict:
        """
        Generate SQL for questions using the pre-generated system prompt.
        
        Args:
            questions: List of question dictionaries
            db_name: Name of the database
            
        Returns:
            Dictionary with predictions and metadata
        """
        results = []
        # Track non-critical failures (critical ones now raise exceptions immediately)
        api_failures = {
            'other_errors': 0
        }
        
        print(f"Processing {len(questions)} questions for database {db_name}")
        print(f"Using model: {self.model_key} ({self.model_name})")
        
        for i, question in enumerate(questions, 1):
            # Track current question for progress reporting in rate limit messages
            self.current_question_num = i
            self.total_questions = len(questions)

            # Handle both dev format (with question_id) and train format (without)
            # For train data, use the original index in the full dataset
            question_id = question.get('question_id', question.get('_original_index', i-1))
            print(f"  Question {i}/{len(questions)}: {question_id}")

            sql, verification_info = self._generate_single_sql(question)

            # Check for critical API failures that should stop processing
            if "API_CREDIT_EXHAUSTION" in sql:
                print(f"\n‚ùå CRITICAL ERROR: API Credit Exhaustion")
                print(f"   Cannot continue - please add credits to your API key")
                raise RuntimeError("API_CREDIT_EXHAUSTION: Cannot continue without API credits")
            elif "API_RATE_LIMIT" in sql:
                print(f"\n‚ùå CRITICAL ERROR: API Rate Limit Hit")
                print(f"   Cannot continue - please wait before retrying")
                raise RuntimeError("API_RATE_LIMIT: Rate limit exceeded, cannot continue")
            elif "API_AUTH_ERROR" in sql:
                print(f"\n‚ùå CRITICAL ERROR: API Authentication Failed")
                print(f"   Cannot continue - please check your API key configuration")
                raise RuntimeError("API_AUTH_ERROR: Authentication failed, cannot continue")
            elif "ERROR:" in sql:
                # Other errors we just track but don't fail immediately
                api_failures['other_errors'] += 1

            result_entry = {
                'question_id': question_id,
                'question': question['question'],
                'evidence': question.get('evidence', ''),
                'predicted_sql': sql,
                'db_id': db_name
            }

            # Add verification info if present
            if verification_info:
                result_entry['verification_info'] = verification_info

            results.append(result_entry)
        
        # Create BIRD format output
        bird_predictions = {}
        for result in results:
            bird_sql = f"{result['predicted_sql']}\t----- bird -----\t{db_name}"
            bird_predictions[str(result['question_id'])] = bird_sql
        
        # Print non-critical failure summary if any occurred
        # Note: Critical API failures (credit/rate/auth) now raise exceptions immediately
        if api_failures['other_errors'] > 0:
            print(f"\n‚ö†Ô∏è  Non-critical failures: {api_failures['other_errors']}/{len(questions)} queries had errors")
            print(f"    These queries returned error messages but processing continued")
        
        # Calculate cost metrics
        cost_without_caching = (
            (self.total_input_tokens + self.cache_creation_tokens + self.cache_read_tokens) / 1000000
        ) * self.pricing['input'] + (self.total_output_tokens / 1000000) * self.pricing['output']
        
        cost_savings = cost_without_caching - self.total_cost
        savings_percentage = (cost_savings / cost_without_caching * 100) if cost_without_caching > 0 else 0
        
        return {
            'predictions': bird_predictions,
            'detailed_results': results,
            'metadata': {
                'database': db_name,
                'prompt_source': 'subagent-generated',
                'generator_version': 'prompt-based-v1',
                'model': {
                    'name': self.model_name,
                    'key': self.model_key,
                    'pricing': self.pricing
                },
                'total_questions': len(questions),
                'token_usage': {
                    'input_tokens': self.total_input_tokens,
                    'output_tokens': self.total_output_tokens,
                    'cache_creation_tokens': self.cache_creation_tokens,
                    'cache_read_tokens': self.cache_read_tokens,
                    'total_cost': self.total_cost,
                    'cost_savings': {
                        'amount': cost_savings,
                        'percentage': savings_percentage
                    }
                },
                'api_failures': api_failures,
                'validation_stats': self.validation_stats,
                'sql_validation_enabled': self.db_path is not None,
                'verification_settings': {
                    'verification_retries': self.verification_retries,
                    'temperature_strategy': self.temperature_strategy,
                    'verification_enabled': self.verification_retries > 0
                },
                'phase2_cache_stats': self.cache_manager.get_cache_stats() if self.cache_manager else {'hits': 0, 'misses': 0, 'total': 0},
                'timestamp': str(datetime.now()),
            }
        }
    
    def _validate_sql(self, sql: str) -> Tuple[bool, Optional[str]]:
        """
        Validate SQL by attempting to execute it against the database.
        
        Args:
            sql: SQL query to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        if not self.sql_executor or not self.db_path:
            return True, None  # Skip validation if not configured
        
        self.validation_stats['validation_attempted'] += 1
        
        try:
            # Attempt to execute the SQL with timeout
            result = self.sql_executor.execute_sql(
                sql=sql,
                db_path=self.db_path,
                is_ground_truth=False,
                timeout_seconds=self.sql_validation_timeout
            )

            # Check if the result is empty (no rows returned)
            if self._is_result_empty(result):
                self.validation_stats['validation_failed_empty'] += 1
                return False, "empty"

            self.validation_stats['validation_passed'] += 1
            return True, None
            
        except SQLTimeoutError as e:
            self.validation_stats['timeout_errors'] += 1
            self.validation_stats['validation_failed'] += 1
            return False, f"SQL execution timed out after {self.sql_validation_timeout}s: {str(e)}"
            
        except Exception as e:
            self.validation_stats['validation_failed'] += 1
            return False, f"SQL validation error: {str(e)}"

    def _is_result_empty(self, result):
        if result is None or len(result) == 0:
            return True
        if result == [[None]] or result == [[]] or result == [[0]]:
            return True
        
        return False

    def _summarize_results(self, results, error_msg=None):
        """
        Summarize SQL execution results for verification prompt.

        Args:
            results: SQL execution results (list of rows)
            error_msg: SQL error message if execution failed

        Returns:
            String summary of results for verification
        """
        if error_msg:
            return f"SQL Error: {error_msg}"

        if results is None or len(results) == 0:
            return "Empty result set (0 rows)"

        # Preserve original order - no set() deduplication!
        if len(results) <= 10:
            return f"Complete results ({len(results)} rows):\n{results}"
        else:
            sample = results[:10]
            return f"Result summary: {len(results)} total rows\nFirst 10 rows:\n{sample}\n[...{len(results)-10} more rows]"

    def _build_verification_prompt(self, question, sql, summary, attempts):
        """
        Build verification prompt with complete history.

        Args:
            question: Question dictionary
            sql: Current SQL query
            summary: Results summary from _summarize_results()
            attempts: List of previous attempt dictionaries

        Returns:
            String prompt for verification
        """
        history = ""
        if attempts:
            history = "Previous attempts:\n\n"
            for i, attempt in enumerate(attempts, 1):
                history += f"Attempt {i}:\n"
                history += f"SQL: {attempt['sql']}\n"
                history += f"Results: {attempt['summary']}\n"
                if attempt.get('feedback'):
                    history += f"Issues identified: {attempt['feedback']}\n"
                history += "\n"

        maybe_evidence = ""
        if self.use_evidence and question.get('evidence'):
            maybe_evidence = f"\nEvidence: {question.get('evidence', 'None')}"

        prompt = f"""{history}Question: {question['question']}{maybe_evidence}

Current SQL: {sql}
Results: {summary}

Review the SQL and results. Does this correctly answer the question?

Respond with EXACTLY one of the following:
1. The single word: CORRECT
2. A new SQL query with no additional text

Do not include explanations, prefixes, or combine both responses.

Your response:"""
        return prompt

    def _get_verification_temperature(self, attempt_num):
        """Get temperature for verification attempt based on strategy."""
        if self.temperature_strategy == "progressive":
            return [0.0, 0.2, 0.3][min(attempt_num, 2)]
        elif self.temperature_strategy == "fixed":
            return 0.0
        elif self.temperature_strategy == "adaptive":
            # Could implement more complex logic later
            return 0.0 if attempt_num == 0 else 0.2
        else:
            return 0.0

    def _verify_and_improve(self, question, sql, summary, attempts):
        """
        Single API call to verify current SQL and potentially get improvement.

        Args:
            question: Question dictionary
            sql: Current SQL query
            summary: Results summary
            attempts: List of previous attempts

        Returns:
            Tuple of (is_correct: bool, new_sql: str, feedback: str)
        """
        verification_prompt = self._build_verification_prompt(question, sql, summary, attempts)
        temperature = self._get_verification_temperature(len(attempts))

        # Check cache before making API call (only for deterministic calls with temperature=0.0)
        cache_key = None
        if self.cache_manager and temperature == 0.0:
            cache_key = self.cache_manager.get_cache_key(
                system_prompt=self.system_prompt,
                user_prompt=verification_prompt,
                model=self.model_name,
                max_tokens=MAX_TOKENS,
                temperature=temperature
            )
            cached_response = self.cache_manager.get_cached_sql(cache_key)
            if cached_response:
                # Cache hit - parse cached response
                if cached_response.upper() == "CORRECT" or cached_response.upper().startswith("CORRECT"):
                    return True, sql, "Model confirmed SQL is correct (cached)"
                else:
                    new_sql = self._clean_sql(cached_response)
                    if new_sql.upper().startswith("CORRECT"):
                        new_sql = new_sql[7:].strip()
                        if new_sql.startswith(':'):
                            new_sql = new_sql[1:].strip()
                        new_sql = self._clean_sql(new_sql)
                    return False, new_sql, "Model provided improved SQL (cached)"

        try:
            # Use provider abstraction for generation
            response = self.provider.generate(
                system_prompt=self.system_prompt,
                user_prompt=verification_prompt,
                temperature=temperature,
                max_tokens=MAX_TOKENS
            )

            # Track token usage from LLMResponse
            self.total_input_tokens += response.input_tokens
            self.total_output_tokens += response.output_tokens
            self.cache_creation_tokens += response.cache_creation_tokens
            self.cache_read_tokens += response.cache_read_tokens

            # Calculate cost
            input_cost = (response.input_tokens / 1000000) * self.pricing['input']
            output_cost = (response.output_tokens / 1000000) * self.pricing['output']
            cache_creation_cost = (response.cache_creation_tokens / 1000000) * self.pricing['cache_write']
            cache_read_cost = (response.cache_read_tokens / 1000000) * self.pricing['cache_read']

            verification_cost = input_cost + output_cost + cache_creation_cost + cache_read_cost
            self.total_cost += verification_cost

            # Parse response
            response_text = response.text

            # Save to cache if cache manager is available and temperature was 0.0
            if self.cache_manager and cache_key and temperature == 0.0:
                self.cache_manager.save_sql_cache(
                    cache_key=cache_key,
                    sql_response=response_text,
                    temperature=temperature,
                    metadata={'model': self.model_name, 'verification': True}
                )

            # Check for various forms of "CORRECT" (handling edge cases)
            if response_text.upper() == "CORRECT" or response_text.upper().startswith("CORRECT"):
                # Debug log this verification API call with response
                self._maybe_debug_log_api_call(
                    api_call_type="verification",
                    question=question,
                    prompt=verification_prompt,
                    api_params={
                        'model': self.model_name,
                        'max_tokens': MAX_TOKENS,
                        'temperature': temperature,
                        'system_prompt': self.system_prompt,  # Full prompt, not just length
                        'cache_control': 'ephemeral',
                        'attempt_num': len(attempts)
                    },
                    response="CORRECT"  # Verification confirmed
                )
                return True, sql, "Model confirmed SQL is correct"
            else:
                # Response should be new SQL - clean it more aggressively
                new_sql = self._clean_sql(response_text)

                # Additional cleaning for edge cases where model includes "CORRECT:" prefix
                if new_sql.upper().startswith("CORRECT"):
                    # Remove "CORRECT:" or "CORRECT " prefix
                    new_sql = new_sql[7:].strip()  # Remove "CORRECT" (7 chars)
                    if new_sql.startswith(':'):
                        new_sql = new_sql[1:].strip()  # Remove colon
                    new_sql = self._clean_sql(new_sql)  # Clean again

                # Debug log this verification API call with improved SQL response
                self._maybe_debug_log_api_call(
                    api_call_type="verification",
                    question=question,
                    prompt=verification_prompt,
                    api_params={
                        'model': self.model_name,
                        'max_tokens': MAX_TOKENS,
                        'temperature': temperature,
                        'system_prompt': self.system_prompt,  # Full prompt, not just length
                        'cache_control': 'ephemeral',
                        'attempt_num': len(attempts)
                    },
                    response=new_sql  # Improved SQL
                )

                return False, new_sql, "Model provided improved SQL"

        except Exception as e:
            error_str = str(e)
            timestamp = datetime.now().strftime("%H:%M:%S")
            db_name = self._get_db_name()

            # For rate limit errors, print concise message (full details shown in retry message)
            if "rate_limit" in error_str.lower():
                print(f"    [{timestamp}] {db_name}: Error in verification: rate limit", flush=True)
            else:
                print(f"    [{timestamp}] {db_name}: Error in verification: {error_str}", flush=True)

            return False, sql, f"Verification failed: {error_str}"

    def _verify_and_improve_with_rate_limit_retry(self, question, sql, summary, attempts):
        """
        Verify and improve SQL with automatic retry on rate limit errors.

        Uses exponential backoff: 60s, 120s, 240s, 480s, then crash.
        Other errors are returned immediately without retry.

        Returns:
            Tuple of (is_correct: bool, new_sql: str, feedback: str)
        """
        for attempt in range(RATE_LIMIT_MAX_RETRIES):
            is_correct, new_sql, feedback = self._verify_and_improve(question, sql, summary, attempts)

            # Check if we hit rate limit (error is in feedback string)
            if "rate_limit" not in feedback.lower():
                # Success or non-retryable error
                return is_correct, new_sql, feedback

            # Rate limit hit - decide whether to retry or give up
            if attempt < RATE_LIMIT_MAX_RETRIES - 1:
                delay = RATE_LIMIT_BASE_DELAY * (2 ** attempt)
                timestamp = datetime.now().strftime("%H:%M:%S")
                db_name = self._get_db_name()

                # Build progress message
                progress = ""
                if self.current_question_num and self.total_questions:
                    progress = f", question {self.current_question_num}/{self.total_questions}"

                print(f"    [{timestamp}] {db_name}: ‚è∏Ô∏è  Verification rate limit{progress} - retry {attempt+1}/{RATE_LIMIT_MAX_RETRIES-1} after {delay}s...", flush=True)
                time.sleep(delay)
            else:
                # Max retries exceeded - return the error which will crash upstream
                timestamp = datetime.now().strftime("%H:%M:%S")
                db_name = self._get_db_name()
                print(f"    [{timestamp}] {db_name}: ‚ùå Verification rate limit persists after {RATE_LIMIT_MAX_RETRIES} retries", flush=True)
                return is_correct, new_sql, feedback

        return is_correct, new_sql, feedback

    def _generate_with_verification(self, question: Dict) -> Tuple[str, Dict]:
        """
        Generate SQL with universal verification and k retries.

        Args:
            question: Question dictionary

        Returns:
            Tuple of (sql_query, verification_info) where verification_info contains
            details about all verification attempts
        """
        self.validation_stats['verification_attempted'] += 1
        attempts = []

        # Initial SQL generation (counts as an API call, with retry on transient errors)
        sql = self._generate_sql_with_retry(question, previous_sql=None, error_msg=None)
        self.validation_stats['generation_api_calls'] += 1
        self.validation_stats['total_api_calls'] += 1

        # Verification loop for k attempts
        for attempt_num in range(self.verification_retries):
            self.validation_stats['verification_attempts_total'] += 1

            # Execute SQL to get results for verification
            if self.sql_executor and self.db_path:
                try:
                    results = self.sql_executor.execute_sql(
                        sql=sql,
                        db_path=self.db_path,
                        is_ground_truth=False,
                        timeout_seconds=self.sql_validation_timeout
                    )
                    summary = self._summarize_results(results)
                except Exception as e:
                    summary = self._summarize_results(None, str(e))
            else:
                # If no validation available, just proceed to next iteration
                summary = "SQL validation not available (no database path provided)"

            print(f"    Verification attempt {attempt_num + 1}/{self.verification_retries}: {summary[:100]}...")

            # Verify and potentially improve (counts as an API call, with rate limit retry)
            is_correct, new_sql, feedback = self._verify_and_improve_with_rate_limit_retry(question, sql, summary, attempts)
            self.validation_stats['verification_api_calls'] += 1
            self.validation_stats['total_api_calls'] += 1

            # Record this attempt
            attempts.append({
                'sql': sql,
                'summary': summary,
                'feedback': feedback,
                'attempt_num': attempt_num,
                'is_correct': is_correct
            })

            if is_correct:
                if attempt_num == 0:
                    # Passed on first verification attempt
                    print(f"    Verification succeeded immediately")
                    self.validation_stats['verification_passed_immediately'] += 1
                    verification_outcome = 'passed_immediately'
                else:
                    # Passed after improvement
                    print(f"    Verification succeeded after {attempt_num} improvement(s)")
                    self.validation_stats['verification_improved'] += 1
                    verification_outcome = 'improved'
                self.validation_stats['verification_succeeded'] += 1

                # Build verification info and return
                verification_info = {
                    'verification_attempts': attempt_num + 1,
                    'verification_outcome': verification_outcome,
                    'final_retry_used': False,
                    'verification_details': attempts
                }
                return sql, verification_info
            else:
                print(f"    Verification attempt {attempt_num + 1} suggested improvement: {feedback}")
                sql = new_sql

        # If all verification attempts failed, use k+1 attempt with current error/null retry logic
        print(f"    All verification attempts failed, using final error/null retry logic")
        self.validation_stats['verification_failed'] += 1

        final_retry_used = False
        final_retry_outcome = None

        # Generate final attempt using current error/null logic (if validation enabled)
        if self.sql_executor and self.db_path:
            is_valid, error_msg = self._validate_sql(sql)
            if not is_valid and error_msg and "non-retryable" not in error_msg:
                print(f"    Final validation failed: {error_msg}, attempting error retry")
                final_retry_used = True

                empty_result = False
                if error_msg == "empty":
                    error_msg = None
                    empty_result = True

                final_sql = self._generate_sql_with_retry(question, previous_sql=sql, error_msg=error_msg, empty_result=empty_result)
                self.validation_stats['total_api_calls'] += 1  # Track fallback API call

                # Validate final attempt
                final_is_valid, final_error_msg = self._validate_sql(final_sql)
                if final_is_valid:
                    print(f"    Final error retry succeeded")
                    final_retry_outcome = 'succeeded'
                    sql = final_sql
                else:
                    print(f"    Final error retry failed: {final_error_msg}")
                    final_retry_outcome = 'failed'
                    sql = final_sql

        # Build verification info for failed case
        verification_info = {
            'verification_attempts': self.verification_retries,
            'verification_outcome': 'failed',
            'final_retry_used': final_retry_used,
            'final_retry_outcome': final_retry_outcome,
            'verification_details': attempts
        }
        return sql, verification_info

    def _generate_single_sql(self, question: Dict) -> Tuple[str, Dict]:
        """Generate SQL for a single question with optional verification or validation and retry.

        Returns:
            Tuple of (sql_query, verification_info)
            where verification_info contains details about verification/retry attempts
        """
        self.validation_stats['total_generated'] += 1

        # Use verification if enabled (verification_retries > 0)
        if self.verification_retries > 0:
            return self._generate_with_verification(question)

        # Otherwise, use current validation and retry logic
        # First attempt - generate SQL (with retry on transient errors)
        sql = self._generate_sql_with_retry(question, previous_sql=None, error_msg=None)
        self.validation_stats['generation_api_calls'] += 1
        self.validation_stats['total_api_calls'] += 1

        # Initialize verification info for legacy path
        verification_info = {
            'verification_attempts': 0,
            'verification_outcome': 'no_verification',
            'legacy_retry_used': False,
            'verification_details': []
        }

        # Validate SQL if validation is enabled
        if self.sql_executor and self.db_path:
            is_valid, error_msg = self._validate_sql(sql)
            print(f"    SQL valid? {is_valid} with {error_msg}")
            if not is_valid and error_msg and "non-retryable" not in error_msg:
                # SQL validation failed with a potentially fixable error - retry once
                print(f"    SQL validation failed: {error_msg} given {sql}")
                print(f"    Retrying with error context...")
                self.validation_stats['retry_attempted'] += 1
                verification_info['legacy_retry_used'] = True
                verification_info['legacy_error'] = error_msg

                empty_result = False
                if error_msg == "empty":
                    error_msg = None
                    empty_result = True

                # Retry with both the failed SQL and error message (with retry on transient errors)
                retry_sql = self._generate_sql_with_retry(question, previous_sql=sql, error_msg=error_msg, empty_result=empty_result)
                self.validation_stats['total_api_calls'] += 1  # Track retry API call

                # Validate the retry attempt
                retry_is_valid, retry_error_msg = self._validate_sql(retry_sql)

                if retry_is_valid:
                    print(f"    Retry succeeded - SQL is now valid, corrected to {retry_sql}")
                    self.validation_stats['retry_succeeded'] += 1
                    verification_info['legacy_retry_outcome'] = 'succeeded'
                    return retry_sql, verification_info
                else:
                    print(f"    Retry failed: {retry_error_msg} with {retry_sql}")
                    verification_info['legacy_retry_outcome'] = 'failed'
                    verification_info['legacy_retry_error'] = retry_error_msg
                    # Return the retry attempt even if it failed validation
                    return retry_sql, verification_info
            elif is_valid:
                print(f"    SQL validation passed")

        return sql, verification_info

    def _generate_sql_with_retry(self, question: Dict, previous_sql: Optional[str] = None,
                                 error_msg: Optional[str] = None, empty_result: bool = False) -> str:
        """
        Generate SQL with automatic retry on transient errors.

        Uses exponential backoff: 60s, 120s, 240s, 480s, then crash.
        Retries on: rate limits, network timeouts, server errors (5xx), model crashes
        Model crashes use fixed 5s delay with 3 retries.
        Other errors (credit exhaustion, auth) are returned immediately without retry.
        """
        MODEL_CRASH_MAX_RETRIES = 3
        MODEL_CRASH_DELAY = 5  # seconds

        for attempt in range(RATE_LIMIT_MAX_RETRIES):
            sql = self._generate_sql_attempt(question, previous_sql, error_msg, empty_result)

            # Check if we hit a retryable error (rate limit, network timeout, server error, or model crash)
            if ("API_RATE_LIMIT" not in sql and "API_NETWORK_TIMEOUT" not in sql and
                "API_SERVER_ERROR" not in sql and "MODEL_CRASH" not in sql):
                # Success or non-retryable error
                return sql

            # Determine error type for messaging
            if "API_RATE_LIMIT" in sql:
                error_type = "Rate limit"
            elif "API_NETWORK_TIMEOUT" in sql:
                error_type = "Network timeout"
            elif "MODEL_CRASH" in sql:
                error_type = "Model crash"
            else:  # API_SERVER_ERROR
                error_type = "Server error"

            timestamp = datetime.now().strftime("%H:%M:%S")
            db_name = self._get_db_name()

            # Special handling for model crashes - use shorter fixed delay and fewer retries
            if "MODEL_CRASH" in sql:
                if attempt < MODEL_CRASH_MAX_RETRIES - 1:
                    progress = ""
                    if self.current_question_num and self.total_questions:
                        progress = f", question {self.current_question_num}/{self.total_questions}"
                    print(f"    [{timestamp}] {db_name}: ‚è∏Ô∏è  {error_type}{progress} - retry {attempt+1}/{MODEL_CRASH_MAX_RETRIES-1} after {MODEL_CRASH_DELAY}s...", flush=True)
                    time.sleep(MODEL_CRASH_DELAY)
                else:
                    # Max retries for model crash - log and skip this question
                    self._log_model_crash(question, sql, MODEL_CRASH_MAX_RETRIES)
                    print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  Skipping question {question.get('question_id')} after {MODEL_CRASH_MAX_RETRIES} model crashes", flush=True)
                    return "SELECT 'MODEL_CRASH_SKIP' AS error"
                continue

            # Retry or give up for other error types
            if attempt < RATE_LIMIT_MAX_RETRIES - 1:
                delay = RATE_LIMIT_BASE_DELAY * (2 ** attempt)

                # Build progress message
                progress = ""
                if self.current_question_num and self.total_questions:
                    progress = f", question {self.current_question_num}/{self.total_questions}"

                print(f"    [{timestamp}] {db_name}: ‚è∏Ô∏è  {error_type}{progress} - retry {attempt+1}/{RATE_LIMIT_MAX_RETRIES-1} after {delay}s...", flush=True)
                time.sleep(delay)
            else:
                # Max retries exceeded - return the error which will crash upstream
                print(f"    [{timestamp}] {db_name}: ‚ùå {error_type} persists after {RATE_LIMIT_MAX_RETRIES} retries", flush=True)
                return sql

        return sql

    def _log_model_crash(self, question: Dict, error: str, attempts: int) -> None:
        """Log model crash to crash_log.json for analysis."""
        try:
            crash_log_path = os.path.join(self.output_dir, "crash_log.json")
            crash_entry = {
                "timestamp": datetime.now().isoformat(),
                "question_id": question.get("question_id"),
                "database": self._get_db_name(),
                "question_text": question.get("question", "")[:200],  # Truncate for readability
                "error": error[:500],  # Truncate long errors
                "attempts": attempts
            }

            # Append to existing crash log
            existing = []
            if os.path.exists(crash_log_path):
                with open(crash_log_path, 'r') as f:
                    existing = json.load(f)
            existing.append(crash_entry)
            with open(crash_log_path, 'w') as f:
                json.dump(existing, f, indent=2)
        except Exception as e:
            # Don't let logging failures crash the main process
            print(f"    Warning: Failed to log model crash: {e}", flush=True)

    def _generate_sql_attempt(self, question: Dict, previous_sql: Optional[str] = None, error_msg: Optional[str] = None, empty_result: bool = False) -> str:
        """Generate a single SQL attempt, optionally with retry context including previous SQL and error."""
        
        maybe_evidence = ""
        if self.use_evidence and question.get('evidence'):
            maybe_evidence = f"\n\nEvidence or hints to help generate the query: {question.get('evidence', 'None provided')}"

        maybe_correct_invalid = ""
        if previous_sql and error_msg:
            maybe_correct_invalid = f"""\n\nYour previous SQL query:
```sql
{previous_sql}
```

This query failed with the following error:
{error_msg}

Please generate a corrected SQL query that fixes this error."""
            
        maybe_correct_empty = ""
        if previous_sql and empty_result and not error_msg:
            maybe_correct_empty = f"""\n\nYour previous SQL query:
```sql
{previous_sql}
```

This query produced no results, which is probably indicative of a problem. Re-examine the question, the previous sql, and 
the database profiling information above to determine if there is a better sql query that may answer the intent better."""

        question_prompt = f"""Question: {question['question']} {maybe_evidence}{maybe_correct_invalid}{maybe_correct_empty}

Return ONLY the SQL query that answers the question above using the database information above, no explanations.

SQL Query:"""

        # Check cache before making API call (only for deterministic calls with temperature=0.0)
        temperature = 0
        if self.cache_manager:
            cache_key = self.cache_manager.get_cache_key(
                system_prompt=self.system_prompt,
                user_prompt=question_prompt,
                model=self.model_name,
                max_tokens=MAX_TOKENS,
                temperature=temperature
            )
            cached_sql = self.cache_manager.get_cached_sql(cache_key)
            if cached_sql:
                # Cache hit - return cached SQL without API call
                return cached_sql

        try:
            # Use provider abstraction for generation
            response = self.provider.generate(
                system_prompt=self.system_prompt,
                user_prompt=question_prompt,
                temperature=temperature,
                max_tokens=MAX_TOKENS
            )

            # Track token usage from LLMResponse
            self.total_input_tokens += response.input_tokens
            self.total_output_tokens += response.output_tokens
            self.cache_creation_tokens += response.cache_creation_tokens
            self.cache_read_tokens += response.cache_read_tokens

            # Calculate cost
            input_cost = (response.input_tokens / 1000000) * self.pricing['input']
            output_cost = (response.output_tokens / 1000000) * self.pricing['output']
            cache_creation_cost = (response.cache_creation_tokens / 1000000) * self.pricing['cache_write']
            cache_read_cost = (response.cache_read_tokens / 1000000) * self.pricing['cache_read']

            question_cost = input_cost + output_cost + cache_creation_cost + cache_read_cost
            self.total_cost += question_cost

            # Extract and clean SQL
            sql = response.text
            sql = self._clean_sql(sql)

            # Save to cache if cache manager is available
            if self.cache_manager:
                self.cache_manager.save_sql_cache(
                    cache_key=cache_key,
                    sql_response=sql,
                    temperature=temperature,
                    metadata={'model': self.model_name}
                )

            # Debug log this API call with response
            self._maybe_debug_log_api_call(
                api_call_type="generation",
                question=question,
                prompt=question_prompt,
                api_params={
                    'model': self.model_name,
                    'max_tokens': MAX_TOKENS,
                    'temperature': 0,
                    'system_prompt': self.system_prompt,  # Full prompt, not just length
                    'cache_control': 'ephemeral'
                },
                response=sql  # Cleaned SQL response
            )

            return sql
            
        except Exception as e:
            error_str = str(e)
            timestamp = datetime.now().strftime("%H:%M:%S")
            db_name = self._get_db_name()

            # Check for specific API errors
            if "insufficient_credits" in error_str.lower() or "credit" in error_str.lower():
                print(f"    [{timestamp}] {db_name}: ‚ùå API CREDIT EXHAUSTION: {error_str}", flush=True)
                raise RuntimeError(f"API_CREDIT_EXHAUSTION: {error_str}")
            elif "authentication" in error_str.lower() or "api_key" in error_str.lower():
                print(f"    [{timestamp}] {db_name}: ‚ùå API AUTHENTICATION ERROR: {error_str}", flush=True)
                raise RuntimeError(f"API_AUTH_ERROR: {error_str}")
            elif "prompt is too long" in error_str.lower() or "maximum context" in error_str.lower():
                print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  CONTEXT OVERFLOW: {error_str}", flush=True)
                print(f"    [{timestamp}] {db_name}: üí° Suggestion: Reduce Phase 1 output size (fewer tables, shorter descriptions)", flush=True)
                return f"CONTEXT_OVERFLOW: {error_str}"
            elif "Read timed out" in error_str or "Operation timed out" in error_str or ("timed out" in error_str.lower() and "connect" not in error_str.lower()):
                # LLM call timeout - our configured llm_call_timeout was exceeded (local models)
                print(f"    [{timestamp}] {db_name}: ‚è±Ô∏è  LLM call timeout after {self.llm_call_timeout}s", flush=True)
                return f"LLM_CALL_TIMEOUT: Exceeded {self.llm_call_timeout}s limit"
            elif "ConnectTimeout" in error_str or "APITimeoutError" in error_str or "connection" in error_str.lower():
                # Network/connection timeout - actual network issues
                print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  Network timeout (connection issue)", flush=True)
                return f"API_NETWORK_TIMEOUT: {error_str}"
            elif ("InternalServerError" in error_str or
                  "500" in error_str or "502" in error_str or "503" in error_str or "504" in error_str or
                  "Bad gateway" in error_str or "Service unavailable" in error_str or "Gateway timeout" in error_str):
                print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  Server error (Anthropic API issue)", flush=True)
                return f"API_SERVER_ERROR: {error_str}"
            elif "rate_limit" in error_str.lower():
                # Extract token limit from error message if present
                import re
                token_limit = None
                if "tokens per minute" in error_str:
                    match = re.search(r'(\d+(?:,\d+)*)\s+(?:input\s+)?tokens per minute', error_str)
                    if match:
                        token_limit = match.group(1)

                # Extract request_id from error string
                request_id = None
                req_match = re.search(r"'request_id':\s*'([^']+)'", error_str)
                if req_match:
                    request_id = req_match.group(1)

                # Try to get token count from exception response headers
                failed_request_tokens = None
                if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                    headers = e.response.headers
                    # Check for various header names that might contain token info
                    for header_name in ['anthropic-ratelimit-tokens-used', 'x-tokens-used', 'anthropic-tokens']:
                        if header_name in headers:
                            try:
                                failed_request_tokens = int(headers[header_name].replace(',', ''))
                            except:
                                pass

                # Build concise rate limit message with question progress
                progress = ""
                if self.current_question_num and self.total_questions:
                    progress = f", question {self.current_question_num}/{self.total_questions}"

                limit_info = f"{token_limit} tokens/min" if token_limit else "rate limit"

                # Add failed request token count and request_id
                token_info = f", request: {failed_request_tokens:,} tokens" if failed_request_tokens else ""
                req_info = f", req: {request_id}" if request_id else ""

                print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  Rate limit ({limit_info}{progress}{token_info}{req_info})", flush=True)
                return f"API_RATE_LIMIT: {error_str}"
            elif "Exit code: 10" in error_str or "Exit code: 11" in error_str or "model has crashed" in error_str.lower():
                print(f"    [{timestamp}] {db_name}: ‚ö†Ô∏è  Model crash detected (LM Studio)", flush=True)
                return f"MODEL_CRASH: {error_str}"
            else:
                print(f"    [{timestamp}] {db_name}: ‚ùå Unknown error generating SQL: {error_str}", flush=True)
                raise RuntimeError(f"Unknown error generating SQL: {error_str}")
    
    def _clean_sql(self, sql: str) -> str:
        """Clean SQL output from any markdown or extra formatting."""
        # Remove markdown code blocks
        sql = re.sub(r'^```sql\s*\n', '', sql)
        sql = re.sub(r'^```\s*\n', '', sql)
        sql = re.sub(r'\n```$', '', sql)
        sql = re.sub(r'```$', '', sql)
        
        # Remove any leading/trailing whitespace
        sql = sql.strip()
        
        # Ensure it ends with semicolon
        if not sql.endswith(';'):
            sql += ';'
        
        return sql


def main():
    parser = argparse.ArgumentParser(description='Generate SQL from questions using pre-generated prompt')
    parser.add_argument('--prompt', required=True, help='System prompt file generated by subagent')
    parser.add_argument('--questions', required=True, help='Questions JSON file')
    parser.add_argument('--db_name', required=True, help='Database name')
    parser.add_argument('--output', required=True, help='Output predictions file')
    parser.add_argument('--model',
                       default=DEFAULT_MODEL,
                       help=f'Model to use. Anthropic: haiku-4.5, sonnet-4.5. Local: ollama/sqlcoder:latest (default: {DEFAULT_MODEL})')
    parser.add_argument('--api_key', help='Anthropic API key')
    parser.add_argument('--limit', type=int, help='Limit number of questions to process')
    parser.add_argument('--no-evidence', action='store_true', 
                       help='Exclude evidence from prompts (for BIRD no-evidence evaluation)')
    parser.add_argument('--db_path', help='Path to database file for SQL validation (enables validation)')
    parser.add_argument('--sql_validation_timeout', type=int, default=30,
                       help='Timeout in seconds for SQL validation (default: 30)')
    parser.add_argument('--verification_retries', type=int, default=2,
                       help='Number of verification attempts (default: 2, 0 = current behavior)')
    parser.add_argument('--temperature_strategy', choices=['progressive', 'fixed', 'adaptive'],
                       default='progressive',
                       help='Temperature strategy for verification retries (default: progressive)')
    parser.add_argument('--debug-log-probability', type=float, default=0.02,
                       help='Probability (0.0-1.0) of logging API calls for debugging (default: 0.02)')
    parser.add_argument('--debug-log-dir', type=str,
                       help='Directory to write debug logs (creates debug/ subdir)')
    parser.add_argument('--run-dir', type=str,
                       help='Research run directory for Phase 2 caching (optional)')
    parser.add_argument('--agent-id', type=str,
                       help='Agent identifier for Phase 2 caching (optional)')
    parser.add_argument('--llm-call-timeout', type=int, default=120,
                       help='Per-call LLM timeout in seconds (default: 120, for local models)')

    args = parser.parse_args()

    # Initialize Phase 2 cache manager if run_dir and agent_id provided
    cache_manager = None
    if args.run_dir and args.agent_id:
        cache_manager = Phase2CacheManager(Path(args.run_dir), args.agent_id)
    
    # Load questions
    with open(args.questions, 'r') as f:
        all_questions = json.load(f)
    
    # Filter questions for this database, preserving original indices
    questions = []
    for idx, q in enumerate(all_questions):
        if q.get('db_id') == args.db_name:
            # Add the original index if not present
            if 'question_id' not in q:
                q['_original_index'] = idx
            questions.append(q)
    
    if args.limit:
        questions = questions[:args.limit]
    
    print(f"\nProcessing {len(questions)} questions for database: {args.db_name}")
    print(f"Using system prompt: {args.prompt}")
    if args.db_path:
        print(f"SQL validation enabled with database: {args.db_path}")
        print(f"SQL validation timeout: {args.sql_validation_timeout} seconds")

    # Show verification settings
    if args.verification_retries > 0:
        print(f"Universal verification enabled: {args.verification_retries} retries with {args.temperature_strategy} temperature strategy")
    else:
        print("Verification disabled (using current validation/retry behavior)")
    
    # Generate SQL
    use_evidence = not args.no_evidence
    generator = PromptBasedSQLGenerator(
        args.prompt,
        args.model,
        args.api_key,
        use_evidence=use_evidence,
        db_path=args.db_path,
        sql_validation_timeout=args.sql_validation_timeout,
        verification_retries=args.verification_retries,
        temperature_strategy=args.temperature_strategy,
        debug_log_probability=args.debug_log_probability,
        debug_log_dir=Path(args.debug_log_dir) if args.debug_log_dir else None,
        cache_manager=cache_manager,
        llm_call_timeout=args.llm_call_timeout
    )
    results = generator.generate_sql(questions, args.db_name)
    
    # Add evidence mode to metadata
    results['metadata']['use_evidence'] = use_evidence
    
    # Save predictions
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Print summary
    print(f"\n‚úì Generated {len(results['predictions'])} SQL predictions")
    print(f"  Evidence mode: {'WITH evidence' if use_evidence else 'WITHOUT evidence'}")
    print(f"  Model: {generator.model_key} ({generator.model_name})")
    print(f"  Total input tokens: {generator.total_input_tokens:,}")
    print(f"  Total output tokens: {generator.total_output_tokens:,}")
    print(f"  Cache creation tokens: {generator.cache_creation_tokens:,}")
    print(f"  Cache read tokens: {generator.cache_read_tokens:,}")
    print(f"  Total cost: ${generator.total_cost:.4f}")
    
    if generator.cache_read_tokens > 0:
        cache_hit_ratio = (generator.cache_read_tokens /
                          (generator.cache_creation_tokens + generator.cache_read_tokens)) * 100
        print(f"  Cache hit ratio: {cache_hit_ratio:.1f}%")

    cost_savings = results['metadata']['token_usage']['cost_savings']
    if cost_savings['percentage'] > 0:
        print(f"  Cost savings from caching: ${cost_savings['amount']:.4f} ({cost_savings['percentage']:.1f}%)")

    # Print Phase 2 cache statistics if cache manager was used
    if cache_manager:
        cache_stats = cache_manager.get_cache_stats()
        if cache_stats['total'] > 0:
            print(f"\n‚úì Phase 2 SQL Cache Statistics:")
            print(f"  Total API calls: {cache_stats['total']}")
            print(f"  Cache hits: {cache_stats['hits']} ({cache_stats['hit_rate']:.1f}%)")
            print(f"  Cache misses: {cache_stats['misses']}")
            print(f"  Estimated savings: ${cache_stats['estimated_savings']:.2f}")
    
    # Print SQL validation statistics if validation was enabled
    if args.db_path and 'validation_stats' in results['metadata']:
        stats = results['metadata']['validation_stats']
        print(f"\n‚úì SQL Validation Statistics:")
        print(f"  Total queries generated: {stats['total_generated']}")

        # Enhanced verification statistics if enabled
        if stats.get('verification_attempted', 0) > 0:
            total_api = stats.get('total_api_calls', 0)
            total_q = stats['total_generated']
            pass_immediate = stats.get('verification_passed_immediately', 0)
            improved = stats.get('verification_improved', 0)
            failed = stats.get('verification_failed', 0)
            verify_calls = stats.get('verification_api_calls', 0)

            api_ratio = f"{total_api/total_q:.2f}x" if total_q > 0 else "N/A"

            print(f"  --- Universal Verification Enabled ---")
            print(f"  API Calls: {total_api} total ({api_ratio} overhead)")
            print(f"  ‚Ä¢ Generation: {stats.get('generation_api_calls', 0)} calls")
            print(f"  ‚Ä¢ Verification: {verify_calls} calls")
            print(f"  Verification Outcomes:")
            print(f"  ‚Ä¢ Passed immediately: {pass_immediate} ({pass_immediate/total_q*100:.1f}%)")
            print(f"  ‚Ä¢ Improved via retry: {improved} ({improved/total_q*100:.1f}%)")
            print(f"  ‚Ä¢ Failed all attempts: {failed} ({failed/total_q*100:.1f}%)")

        # Legacy validation stats (always shown if validation is enabled)
        if stats['validation_attempted'] > 0 or stats['retry_attempted'] > 0:
            print(f"  --- Legacy Syntax Validation ---")
            print(f"  Validation attempted: {stats['validation_attempted']}")
            print(f"  Validation passed: {stats['validation_passed']}")
            print(f"  Validation failed: {stats['validation_failed']}")
            if stats['retry_attempted'] > 0:
                print(f"  Retries attempted: {stats['retry_attempted']}")
                print(f"  Retries succeeded: {stats['retry_succeeded']}")
                retry_success_rate = (stats['retry_succeeded'] / stats['retry_attempted']) * 100 if stats['retry_attempted'] > 0 else 0
                print(f"  Retry success rate: {retry_success_rate:.1f}%")

        if stats.get('timeout_errors', 0) > 0:
            print(f"  Timeout errors: {stats['timeout_errors']}")
    
    print(f"  Output saved to: {args.output}")


if __name__ == "__main__":
    main()