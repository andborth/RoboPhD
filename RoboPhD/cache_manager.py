"""
Phase 1 analysis caching for RoboPhD system.

This module implements run-scoped caching to prevent redundant database analyses.
When the same agent analyzes the same database, we can reuse the cached output,
saving ~$3-5 per redundant analysis.
"""

import hashlib
import json
import sqlite3
import threading
from pathlib import Path
from typing import Optional, Dict


class CacheManager:
    """
    Manages run-scoped Phase 1 analysis caching.

    Cache is specific to a single research run (identified by run_dir).
    Cache keys are computed from agent artifacts + database schema.
    Only successful Phase 1 results are cached.
    """

    def __init__(self, run_dir: Path):
        """
        Initialize cache manager for a research run.

        Args:
            run_dir: Research run directory (e.g., research/parallel_agent_20250929_...)
        """
        self.run_dir = Path(run_dir)
        # All agents (initial and evolved) are stored in the experiment's agents directory
        self.agents_directory = self.run_dir / "agents"
        self.cache_dir = self.run_dir / "cache" / "phase1_analysis"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.index_file = self.cache_dir / "index.json"
        self.index = self._load_index()

        # Statistics
        self.hits = 0
        self.misses = 0

        # Thread safety for parallel execution
        self._index_lock = threading.Lock()

    def _load_index(self) -> Dict:
        """Load cache index from disk."""
        if self.index_file.exists():
            with open(self.index_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_index(self):
        """Save cache index to disk (thread-safe)."""
        with self._index_lock:
            with open(self.index_file, 'w') as f:
                json.dump(self.index, f, indent=2)

    def get_phase1_cache_key(self, agent_name: str, db_name: str, db_path: Path) -> str:
        """
        Generate stable hash for agent + database combination.

        Hash includes:
        - All files in agent directory (agent.md, eval_instructions.md, tools/*)
        - Database schema (table definitions)
        - Database name (for clarity in index)

        Args:
            agent_name: Name of the agent
            db_name: Name of the database
            db_path: Path to the database file

        Returns:
            SHA256 hash as hex string
        """
        agent_dir = Path(self.agents_directory) / agent_name

        if not agent_dir.exists():
            raise ValueError(f"Agent directory not found: {agent_dir}")

        # Collect all agent files in deterministic order
        file_contents = []
        for file_path in sorted(agent_dir.rglob("*")):
            if file_path.is_file():
                relative_path = file_path.relative_to(agent_dir)
                try:
                    content = file_path.read_text()
                    file_contents.append(f"{relative_path}:{content}")
                except Exception as e:
                    # Skip binary files or unreadable files
                    continue

        # Get database schema
        schema = self._get_database_schema(db_path)

        # Combine everything
        combined = "\n".join(file_contents) + f"\n{db_name}\n{schema}"

        return hashlib.sha256(combined.encode()).hexdigest()

    def _get_database_schema(self, db_path: Path) -> str:
        """
        Extract database schema for cache key computation.

        Args:
            db_path: Path to SQLite database

        Returns:
            Concatenated CREATE statements
        """
        if not db_path.exists():
            raise ValueError(f"Database not found: {db_path}")

        try:
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Get all table creation SQL
            cursor.execute(
                "SELECT sql FROM sqlite_master WHERE type='table' AND sql IS NOT NULL ORDER BY name"
            )

            schemas = [row[0] for row in cursor.fetchall()]
            conn.close()

            return "\n".join(schemas)
        except Exception as e:
            raise ValueError(f"Failed to extract schema from {db_path}: {e}")

    def get_phase1_cache(self, cache_key: str) -> Optional[str]:
        """
        Retrieve cached Phase 1 output.

        Args:
            cache_key: Hash generated by get_phase1_cache_key

        Returns:
            Content of agent_output.txt if cached, None if cache miss
        """
        if cache_key in self.index:
            cached_file = self.cache_dir / cache_key / "agent_output.txt"
            if cached_file.exists():
                self.hits += 1
                return cached_file.read_text()

        self.misses += 1
        return None

    def save_phase1_cache(self, cache_key: str, output_content: str,
                         agent_name: str = None, db_name: str = None) -> None:
        """
        Save successful Phase 1 output to cache.

        Args:
            cache_key: Hash generated by get_phase1_cache_key
            output_content: Content of agent_output.txt
            agent_name: Optional agent name for index metadata
            db_name: Optional database name for index metadata
        """
        # Create cache entry directory
        cache_entry_dir = self.cache_dir / cache_key
        cache_entry_dir.mkdir(parents=True, exist_ok=True)

        # Save output file
        output_file = cache_entry_dir / "agent_output.txt"
        output_file.write_text(output_content)

        # Update index (thread-safe: acquire lock before modifying and saving)
        with self._index_lock:
            self.index[cache_key] = {
                'agent_name': agent_name,
                'db_name': db_name,
                'output_file': str(output_file.relative_to(self.run_dir))
            }
            # Save inside lock to ensure atomicity (modification + save together)
            with open(self.index_file, 'w') as f:
                json.dump(self.index, f, indent=2)

    def get_cache_stats(self) -> Dict:
        """
        Get cache performance statistics.

        Returns:
            Dict with hits, misses, total, hit_rate, estimated_savings
        """
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0

        # Estimate savings: ~$4 per cached Phase 1 analysis
        estimated_savings = self.hits * 4.0

        return {
            'hits': self.hits,
            'misses': self.misses,
            'total': total,
            'hit_rate': hit_rate,
            'estimated_savings': estimated_savings
        }